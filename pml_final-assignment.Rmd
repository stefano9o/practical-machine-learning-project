---
title: "A machine learnig algorithm for activity recognition"
author: "Stefano Galeano"
date: "28/12/2017"
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This work is the final assignment for the "Practical Machine Learning" course administered by Jhon Hopkins University. The material provided for the assignment came from http://groupware.les.inf.puc-rio.br/har, where the authors proposed a model-based approach for recognizing how well an activity it's performed (QAR). In the paper, they compared their work with a machine learning algorithm for mistake detection, demonstrating the advantages in using the model-based approach. In this work, we're going to use the data provided by the authors, in order to create a simple machine learning algorithm for human activity recognition (HAR).

The work is organized as follow: in the first section, we're going to retrieve the data from the source and extract the interesting features. In the second section, we're going to perform some basic exploratory analysis, and in the last section, we'll estimate an out-of-sample error of the ML algorithm, using a fresh and never used dataset.

## Getting and cleaning data

The train/test datasets are downloaded, saved locally and loaded in R treating the values ```"NA","#DIV/0!",""``` as missing values:

```{r, message=TRUE, warning=TRUE, cache=TRUE}
trainUrl <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
testUrl <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'

download.file(url = trainUrl,destfile = 'data/train.csv')
download.file(url = testUrl,destfile = 'data/test.csv')
train <- read.csv('data/train.csv', header = TRUE,
                  stringsAsFactors = FALSE, na.strings = c("NA","#DIV/0!",""))
test <- read.csv('data/test.csv', header = TRUE,
                 stringsAsFactors = FALSE, na.strings = c("NA","#DIV/0!",""))
```

The dataset is composed of: 

* **raw features**: recorded by four 9-dof Razor inertial measurement units (IMU), which provide three-axis acceleration, gyroscope and magnetometer data at a joint sampling rate of 45 Hz. Each IMU was mounted on users' glove, armband, lumbar belt and dumbbell for a total of 36 features;
* **extracted features**: calculated from the raw features using a sliding windows approach. For each temporal window, summarizing features like *mean*, *variance*, *max*, etc. have been calculated for a total of 96 derived features.
* **classe**: represent the type of activity performed. Class A corresponds to the specified execution of the exercise (Unilateral Dumbbell Biceps Curl), while the other 4 classes correspond to common mistakes. 

In http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201 has been shown that using a subset of the extracted features leads to good results (97% accuracy); however, in the context of this assignment, we're going to use only the raw features, and this is because the test dataset provided for the final quiz, doesn't include any extracted features:

```{r}
library(caret, quietly = TRUE)

rawFeatures <- grep(pattern = "^(accel|gyros|magnet)_.*_(x|y|z)$",x = names(train))
rawFeatures <- c(rawFeatures,grep(pattern = "^(roll|pitch|yaw)_.*",x = names(train)))
rawFeatures <- c(rawFeatures,grep(pattern = "^total_accel.*",x = names(train)))

ds.sel <- train[,rawFeatures]
ds.sel$classe <- as.factor(train$classe)
```

```{r}
rawFeatures <- grep(pattern = "^(accel|gyros|magnet)_.*_(x|y|z)$",x = names(test))
rawFeatures <- c(rawFeatures,grep(pattern = "^(roll|pitch|yaw)_.*",x = names(test)))
rawFeatures <- c(rawFeatures,grep(pattern = "^total_accel.*",x = names(test)))
quiz.sel <- test[,rawFeatures]
```

The data are then split into train and test set:

```{r}
set.seed(2015)
inTrain <- createDataPartition(ds.sel$classe,p=.75,list = FALSE)

train.sel <- ds.sel[inTrain,]
test.sel <- ds.sel[-inTrain,]
```

## Exploratory data analysis

First of all, we are going to remove from the training dataset all the variables with a correletion higher then `0.75`: 

```{r}
isFactor <- sapply(train.sel,is.factor)
train.cor <- cor(train.sel[,-which(isFactor)])
highlyCorrelated <- findCorrelation(train.cor, cutoff=0.75, names = TRUE)
train.sel[,highlyCorrelated] <- NULL
```

and we the remaining variables we are going to train a model. We are interested in having an accuracy as high as possible, and one of the best models for this is *random forset*. It's well known that this method leads to overfitting, so we are going to use a *10-fold* cross-validation. Because of the cross-validation and the high number of observations and predictors, we're going to enable the multi-core computation:

```{r cache=TRUE}
library(parallel)
library(doParallel)

set.seed(1234)
seeds <- vector(mode = "list", length = 11) #length is = (n_repeats*nresampling)+1
for(i in 1:10) #(3 is the number of tuning parameter, mtry for rf)
  seeds[[i]] <- sample.int(n=1000, 3) # here equal to 3 (mtry=2,mtry=17, and mtry=34)
seeds[[11]] <- sample.int(1000, 1) # For the last model:

cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

control <- trainControl(method="cv", number=10,allowParallel = TRUE,seeds = seeds)
model <- train(classe~., data=train.sel, method="rf", trControl=control)
stopCluster(cluster)
registerDoSEQ()

model
```

The most important parameters to be tuned for the `rf` method are: the number of the tree to grow (`ntree`), and the number of predictors to be taken into account as potential splitting variable at each split (`mtry`). By default, the `train` function keeps fixed the first parameter (`ntree=500`), while it varies the second; in our case, it assumes three different values (`mtry=2`,`mtry=17`, and `mtry=34`). From the results above, it turns out that `mtry = 2` produces the best results.

Using the model just trained, we're going to calculate the variable importances:

```{r}
importance <- varImp(model, scale=FALSE)
ord <- order(importance$importance,decreasing = TRUE)

plot(importance)
```

From the plot it turns out that `yaw_belt` is the most important variable; this variable conveys the change in direction of the sensor belt (see here for more information about yaw rotation https://en.wikipedia.org/wiki/Yaw_rotation). This sounds a little bit strange since the subjects weren't supposed to do any rotation with the pelvis. In order to understand this behaviour, we're going to plot the temporal variable `raw_timestamp_part_1` against the `yaw_belt` and `username` variable:

```{r}
km <- kmeans(train$raw_timestamp_part_1,centers = 20)

df <- data.frame(time = as.factor(km$cluster), yaw_belt = train$yaw_belt, subject = as.factor(train$user_name))
g <- ggplot(df,mapping = aes(x=time,y=yaw_belt,color=subject))
# g <- g + ylim(0.7, 1)
g <- g + geom_jitter()
g <- g + xlab("time")
g <- g + ylab("yaw belt")
g
```

As we can see from the plot, there is a clear relationship among all the three variables, and in particular, the `raw_belt` variable highly depends on the session in which the exercise was made. We can also note how the variable in each session, doesn't have a big variance as the whole variable has. In this context, the `raw_belt` can be interpreted as the initial orientation of the subject.

Returning to the plot, even if the importance decreases relatively quickly we don't want to remove any predictors. Indeed, as explained in [https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-requiredModelAccuracy.md], in order to achieve a 20/20 rights predictions with the quiz dataset provided, we need to have a very high accuracy. 

## Results

We can now predict the `classe` variable using the test dataset and see where our model behaves well:

```{r}
pred <- predict(model,newdata = test.sel)
confMatrix <- confusionMatrix(data = pred,test.sel$classe)
print(confMatrix$overall)
confMatrix$table
```

In the test dataset, we have a '99.23%' of accuracy, which is a relatively high value and it should guarantee us a 20/20 right prediction in the quiz dataset.
Since the test dataset has never been used so far, we can interpret the error, as a good estimator of the Out-of-sample error.

Finally we can predict the classe variable for the quiz dataset:

```{r}
pred <- predict(model,newdata = quiz.sel)
pred
```